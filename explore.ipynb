{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "\n",
    "Let's explore the interactions of all attributes and target variable to help discover drivers of our target variable. \n",
    "\n",
    "> \"Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns, to spot anomalies, to test hypotheses, and to check assumptions with the help of summary statistics and graphical representations.\" - Prasad Patil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import env\n",
    "import wrangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire and Prepare Data\n",
    "\n",
    "We will use the function we created and stored in our wrangle file to quickly acquire and prepare our student grades data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our function from wrangle to acquire and prepare our data.\n",
    "\n",
    "df = \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No duplicate student_id values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 3))\n",
    "\n",
    "# List of columns\n",
    "\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    \n",
    "    # i starts at 0, but plot nos should start at 1\n",
    "    \n",
    "    # Create subplot.\n",
    "    \n",
    "    # Title with column name.\n",
    "    \n",
    "    # Display histogram for column.\n",
    "    \n",
    "    # Hide gridlines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Distribution Takeaways\n",
    "\n",
    "- All of the column distributions are bimodal. There seem to be more students scoring on the upper and lower edges than in the middle 80s.\n",
    "- `exam3` has the highest median score, `exam1` and `final_grade` look to have the same or very similar medians, and `exam2` has the lowest median score.\n",
    "- `exam2` has the least students scoring in the upper half of the grade range and the most scoring in the lower half.\n",
    "- `exam1` and `final_grade` distributions look very similar in these initial charts although `exam1` has a larger range in scores than `final_grade`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Stages in Exploration\n",
    "\n",
    "Let's check out the main stages in performing Exploratory Data Analysis, EDA, and run through some guidance for each stage. \n",
    "\n",
    "> “A hypothesis may be simply defined as a guess. A scientific hypothesis is an intelligent guess.” – Isaac Asimov\n",
    "\n",
    "**Hypothesize:** Form and document your initial hypotheses about how the predictors (independent variables, features, or attributes) interact with the target (y-value or dependent variable).\n",
    "- Document your initial hypotheses. Write them down so they're concrete and not in your head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize:** Use visualization techniques (scatterplot, jointplot, pairgrid, heatmap) to identify drivers. Sometimes a visualization is so powerful that it can suffice in identifying a driver all on its own. Other times, a visualization needs to be followed up with a statistical test. When in doubt, follow up with the appropriate test. \n",
    "- Plot out the distributions of each feature. *This is critical b/c many of our statisitical tools and machine learning algorithms assume certain distributions. If your data isn't remotely normally distributed, then avoid using any tools that assume normally distributed data.*\n",
    "- Plot out the interaction of 2 or more variables.\n",
    "- Plot out how subgroups compare to each-other and to the overall population.\n",
    "- Document any surprises you may find in visualizing. This means write down your takeaways; documenting your takeaways is a huge component of your final deliverable/analysis. \n",
    "- Identitfy features that correlate with each other. If feature A and feature B are each tightly correlated with the target variable, but they're also tightly correlated with each other, we should use one feature that correlates better, rather than use both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Your Hypotheses:** Analyze the drivers of a continuous target variable using the appropriate statistical tests (t-tests, correlation, and chi-squared hypothesis tests).\n",
    "> \"Hypothesis generation is a process beginning with an educated guess whereas hypothesis testing is a process to conclude that the educated guess is true/false or the relationship between the variables is statistically significant or not.\" [source](https://www.analyticsvidhya.com/blog/2020/09/hypothesis-generation-data-science-projects/)\n",
    "- Document your hypothesis test results. That means writing up when the tests reject the null hypothesis or fail to reject your null hypothesis for each hypothesis you make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Visualizations\n",
    "\n",
    "Here is a breakdown of visualization by type with some code useful snippets. Below, let's use the appropriate visualizations on our student grades dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Univariate Distributions**\n",
    "    - Check out the distributions of a single variable at a time using pandas built-in plotting function to create a historgram or Seaborn `displot`, `boxplot`, or `countplot`; this can be done before splitting our data if we want.\n",
    "    - **Continuous variable distributions**\n",
    "    ```python\n",
    "     df.[col].hist(grid=False, bins=10)\n",
    "     sns.displot(x, data)\n",
    "     sns.boxplot(data)\n",
    "     ```\n",
    "     - **Discrete variable distributions**\n",
    "     ```python\n",
    "     sns.countplot(x='discrete_var', data)\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Continuous with Continuous**\n",
    "    - Seaborn `pairplot` to create a scatter matrix visualizing all continous variable relationships along with individiual distributions.\n",
    "    ```python\n",
    "    sns.pairplot(data)\n",
    "    ```\n",
    "    - Seaborn `relplot` for a simple scatter plot of two continuous variables.\n",
    "    ```python\n",
    "    sns.relplot(x, y, data, kind=scatter)\n",
    "    ```\n",
    "    - Seaborn `lmplot` for a simple scatter plot of two continous variables with a regression line. I can pass a discrete variable to `col` or `hue` to bring in another dimension, too.\n",
    "    ```python\n",
    "    sns.lmplot(x, y, data, scatter=True, hue=None, col=None)\n",
    "    ```\n",
    "    - Seaborn `jointplot` for a simple scatter plot of two continuous variables with a regression line and the addition of a histogram for each variable.\n",
    "    ```python\n",
    "    sns.jointplot(x, y, data, kind=scatter)\n",
    "    ```\n",
    "    - Seaborn `heatmap` of Correlation Coefficients for all numeric columns in a dataset.\n",
    "    ```python\n",
    "    sns.heatmap(train.corr())\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Discrete with Continuous**\n",
    "    - Seaborn `swarmplot` or `stripplot` to examine a discrete variable by a continuous.\n",
    "    ```python\n",
    "    sns.swarmplot(x='discrete_var', y='continuous_var', data=train)\n",
    "    sns.stripplot(x='discrete_var', y='continuous_var', data=train)\n",
    "    ```\n",
    "    - Seaborn `boxplot`, `violinplot`, or `barplot` to show the distribution of a continuous variable by a discrete variable.\n",
    "    ```python\n",
    "    sns.boxplot(x='discrete_var', y='continuous_var', data=train)\n",
    "    sns.violinplot(x='discrete_var', y='continuous_var', data=train)\n",
    "    sns.barplot(x='discrete_var', y='continuous_var', data=train)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Discrete with Discrete**\n",
    "    - Seaborn `heatmap` with a pandas `crosstab` to examine discrete variables with discrete.\n",
    "    ```python\n",
    "    ctab = pd.crosstab(index, columns, values)\n",
    "    sns.heatmap(ctab, annot=True)\n",
    "    ```\n",
    "\n",
    "Let's take a look at some common chart types by variable types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "\n",
    "Before we explore bi- and multi-variate relationships, we *must* split our data to avoid leakage of unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, validate, and test sets; notice that we are keeping X and Y together so far.\n",
    "\n",
    "train, validate, test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Let's keep our goal from our student grades scenario in mind here.\n",
    "> I'm a university professor hoping I can build a prediction model that will be able to use these exams to predict the final grade within 5 points average per student.\n",
    "\n",
    "Since my target variable is continuous, `final_grade`, this is a regression problem. It's important to remember that Multiple linear regression analysis makes several key assumptions:\n",
    "\n",
    "- There must be a linear relationship between the outcome variable and the independent variables.  *Scatterplots can show whether there is a linear or curvilinear relationship.*\n",
    "- No Multicollinearity—Multiple regression assumes that the independent variables are not highly correlated with each other.\n",
    "- Multivariate Normality–Multiple regression assumes that the residuals are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesize\n",
    "\n",
    "- My Null Hypothesis is that there is no correlation between the grades for `exam1` and `final grade`.\n",
    "- My Alternative Hypothesis is that `exam1`is correlated with `final grade`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and Test\n",
    "\n",
    "- To test my hypothesis, I'm going to create some visualizations and test statistics with my student grades data.\n",
    "- At the same time, I'll be checking that the key assumptions for multiple linear regression are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sns.heatmap()`\n",
    "\n",
    "Let's look at a heatmap of the correlation coefficients for a dataset. [Here](https://towardsdatascience.com/all-about-heatmaps-bb7d97f099d7 ) is an aritcle with lots of heatmap customization options.\n",
    "\n",
    "- First, I need to calculate the correlation coefficient for each pair of variables.\n",
    "- Pandas `.corr()` method allows me to quickly create a correlation matrix by computing pairwise correlation of columns. By default, `method=pearson`.\n",
    "- I can change the `.corr()` argument to `method=spearman` if my variables are not normally distributed. Want to know more about the difference between pearson's r and spearman's rank? [This article](https://towardsdatascience.com/clearly-explained-pearson-v-s-spearman-correlation-coefficient-ada2f473b8) is short, sweet, and to the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the correlation matrix for all exams.\n",
    "\n",
    "exam_corr = \n",
    "exam_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, I pass my correlation matrix to Seaborn's `heatmap` along with any customization I want to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass my correlation matrix to Seaborn's heatmap.\n",
    "\n",
    "kwargs = {'alpha':.9,'linewidth':3, 'linestyle':'-', \n",
    "          'linecolor':'k','rasterized':False, 'edgecolor':'w', \n",
    "          'capstyle':'projecting',}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a scipy stats function pearsonr to calculate the correlation coefficient and the p-value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since my variables are not normally distributed, I might choose Spearman instead.\n",
    "\n",
    "exam_spearman = \n",
    "exam_spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My numbers come out pretty close to the same here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a scipy stats function spearmanr to calculate the correlation coefficient and the p-value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap Takeaways\n",
    "\n",
    "- Although all of the exams have very high positive correlations with the target variable, `exam1` and `final_grade` are almost perfectly correlated. This looks to be the best predictor of our target variable.\n",
    "- Based on my correlation coefficient and my p-value, **I reject my Null hypothesis that there is no correlation between `exam1` and `final_grade`.**\n",
    "- Looking at the correlation between our independent variables, they also have high positive correlations with each other, multicollinearity. This informs me that I don't want to use all of them together in a linear regression model. \n",
    "- I will choose `exam1` and perform a simple linear regression first. If I want to go back and do some feature engineering with my other independent variables, I could do that and see if I can improve on my accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sns.relplot()`\n",
    "\n",
    "Let's do a simple scatter plot of two continuous variables in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sns.lmplot()`\n",
    "\n",
    "Let's make that simple scatter plot but add a regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can really pop that line color if I want.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sns.jointplot()`\n",
    "\n",
    "Let's use a `sns.jointplot()` with `kind=reg` to view individual variable distributions for our x and y along with a scatter plot with regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways\n",
    "\n",
    "- My `relplot`, `lmplot`, and `joinplots` charts show me that there is a linear relationship between `exam1` and `final_grade`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sns.pairplot()`\n",
    "\n",
    "Let's use `sns.pairplot()` to view a scatter plot visualizing the relationships between all of the numeric columns in our dataset all at once as well as individual distributions for each individual column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can drop the redundant information in the upper right half of the chart if we like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional thoughts\n",
    "- Identify if there are logical/domain/cultural cutoffs in continuous variables that would allow us to treat them as categorial values. For example, 98.45 and 99.1 are both an A or an A+ grade in most scales.\n",
    "- If there's a logical cutoff point, like a grade of 70 or a voting age of 18, we can make a boolean to go along with a continuous value. This can allow us to gain additional insight in visualizing distributions between groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{(train.exam1 < 70).sum()} students failed exam1.')\n",
    "print(f'{(train.exam2 < 70).sum()} students failed exam2.')\n",
    "print(f'{(train.exam3 < 70).sum()} students failed exam3.')\n",
    "print(f'{((train.exam1 < 70) & (train.exam2 < 70)).sum()} students failed exam1 and exam2.')\n",
    "print(f'{(train.final_grade < 70).sum()} students failed the final.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_1 = \n",
    "failed_2 = \n",
    "failed_1_and_2 = \n",
    "failed_1_or_2 = \n",
    "failed_final = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What % of students who failed exams 1 and 2 failed the final.\n",
    "\n",
    "failed_1_and_2.isin(failed_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_1_not_2 = \n",
    "# print(f'{len(failed_1_not_2)} students failed exam1 but not exam2.')\n",
    "# failed_1_not_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_2_not_1 = \n",
    "print(f'{len(failed_2_not_1)} students failed exam2 but not exam1.')\n",
    "failed_2_not_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students who only failed exam1 or exam2 passed the final.\n",
    "\n",
    "failed_1_or_2_df = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Thoughts Takeaways\n",
    "\n",
    "- I could create a new feature of boolean values to identify students who failed exam1 and exam2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Visualization with Seaborn Demos](https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html)\n",
    "- <https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15>\n",
    "- <https://www.itl.nist.gov/div898/handbook/index.htm>\n",
    "- <https://adataanalyst.com/data-analysis-resources/visualise-categorical-variables-in-python/>\n",
    "- Boxplot vs. Violin example https://matplotlib.org/3.2.1/gallery/statistics/boxplot_vs_violin.html\n",
    "- https://datavizcatalogue.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises I - Required\n",
    "\n",
    "Our Telco scenario continues:\n",
    "\n",
    "As a customer analyst for Telco, you want to know who has spent the most money with the company over their lifetime. You have monthly charges and tenure, so you think you will be able to use those two attributes as features to estimate total charges. You need to do this within an average of $5.00 per customer. \n",
    "\n",
    "In these exercises, you will run through the stages of exploration as you continue to work toward the above goal.\n",
    "\n",
    "Do your work in a notebook named `explore.ipynb`. In addition, you should create a file named `explore.py` that contains the following functions for exploring your variables (features & target).\n",
    "\n",
    "1. Make sure to perform a train, validate, test split before and use only your train dataset to explore the relationships between independent variables with other independent variables or independent variables with your target variable.\n",
    "\n",
    "1. Write a function named `plot_variable_pairs` that accepts a dataframe as input and plots all of the pairwise relationships along with the regression line for each pair.\n",
    "\n",
    "1. Write a function named `months_to_years` that accepts your telco churn dataframe and returns a dataframe with a new feature `tenure_years`, in complete years as a customer.\n",
    "\n",
    "1. Write a function named `plot_categorical_and_continuous_vars` that accepts your dataframe and the name of the columns that hold the continuous and categorical features and outputs 3 different plots for visualizing a categorical variable and a continuous variable.\n",
    "\n",
    "1. Save the functions you have written to create visualizations in your `explore.py` file. Rewrite your notebook code so that you are using the functions imported from this file.\n",
    "\n",
    "1. Explore your dataset with any other visualizations you think will be helpful.\n",
    "\n",
    "1. In a seperate notebook, use the functions you have developed in this exercise with the `mall_customers` dataset in the Codeup database server. You will need to write a sql query to acquire your data. Make `spending_score` your target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises II - Challenge\n",
    "\n",
    "Our Zillow scenario continues:\n",
    "\n",
    "As a Codeup data science graduate, you want to show off your skills to the Zillow data science team in hopes of getting an interview for a position you saw pop up on LinkedIn. You thought it might look impressive to build an end-to-end project in which you use some of their Kaggle data to predict property values using some of their available features; who knows, you might even do some feature engineering to blow them away. Your goal is to predict the values of single unit properties using the observations from 2017.\n",
    "\n",
    "In these exercises, you will run through the stages of exploration as you continue to work toward the above goal.\n",
    "\n",
    "1. Use the functions you created above to explore your Zillow train dataset in your `explore.ipynb` notebook.\n",
    "\n",
    "1. Come up with some initial hypotheses based on your goal of predicting property value.\n",
    "\n",
    "1. Visualize all combinations of variables in some way.\n",
    "\n",
    "1. Run the appropriate statistical tests where needed.\n",
    "\n",
    "1. What independent variables are correlated with the dependent variable, home value?\n",
    "\n",
    "1. Which independent variables are correlated with other independent variables (bedrooms, bathrooms, year built, square feet)?\n",
    "\n",
    "1. Make sure to document your takeaways from visualizations and statistical tests as well as the decisions you make throughout your process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
